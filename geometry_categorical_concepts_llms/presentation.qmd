---
title: "The Geometry of Categorical and Hierarchical Concepts in Large Language Models"
author: "Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch"
format:
  revealjs:
    theme: default
    css: styles.css
    self-contained: false  # Important! So external scripts can load
    include-in-header: mathjax.html
execute:
  kernel: myenv
---

# Background

## LLM Background

![](llm_background.png)

:::: {.columns}

::: {.column width="25%"}
#### Embedding
$\lambda(x) \in \mathbb{R}^d$
:::

::: {.column width="50%"}
#### Softmax
$\mathbb{P}(y \mid x) \propto \exp(\lambda(x)^\top \gamma(y))$
:::

::: {.column width="25%"}
#### Unembedding
$\gamma(y) \in \mathbb{R}^d$
:::

::::

::: footer
Source: [Author's presentation](https://kihopark.github.io/files/NeurIPS%202023%20Workshop%20keynote.pdf)
:::

## LLM Background: A Visual Representation

![](llm_background_draw.png){fig-align="center"}



## Linear Representation Hypothesis

The *linear representation hypothesis* suggests that semantic concepts are linear directions in the model's representation space.

![](linear_rep.png){fig-align="center"}

::: footer
Source: [Author's presentation](https://kihopark.github.io/files/NeurIPS%202023%20Workshop%20keynote.pdf)
:::

## Causally Separable Concepts

![](causally_separable.png)

Intuition: If two concepts are causally separable, modifying one would not modify the other one.

::: footer
Source: [Author's presentation](https://kihopark.github.io/files/NeurIPS%202023%20Workshop%20keynote.pdf)
:::

## Causal Inner Product Intuition

![](causal_ip.png){fig-align="center"}

::: {.tiny-text}

- $\lambda$ is in the embedding space.
- $\gamma$ is in the unembedding space.
- Intuition: causal inner product induces a linear transformation for the representation spaces s.t. causally separable concepts are orthogonal.
- The causal inner product unifies $\bar{g}_W$ (unembedding representation of $W$) and $\bar{l}_W$ (embedding representation of $W$).
:::

::: footer
Source: Park, K., Choe, Y. J., & Veitch, V. (2023)
:::

## Preliminaries: Inner Product

- Symmetric, positive definite (P.D.), bilinear form $\langle u,v \rangle$.
- Positive definite defn.: $\forall u, v \neq 0,\langle u,v \rangle > 0$.
- Inner product $\langle u,v \rangle = u^T G v$.

  ::: {.tiny-text}
  - $G$ is symmetric, P.D, meaning that it has only positive eigenvalues.
  - $G = PDP^T = (PD^{\frac{1}{2}})(D^{\frac{1}{2}}P^T) = XX^T$
  - Then, we can use $\langle u,v \rangle = (u^T X)(X^T v) = (X^T u)^T (X^T v) = \bar{u}^T \bar{v}$. 
    - This means that we are mapping the vectors to a new space, and use the Euclidian inner product in the new space.
  :::
<!-- - When $G = I_d$, we have the Euclidian inner product. -->

<!-- ::: footer
Source: Mark Crovella lecture notes.
::: -->

## Defining the Causal Inner Product

- There is some invertible matrix $A$ and constant vector $\bar{\gamma}_0$ such that, if we transform the embedding and unembedding spaces as:
  $$g(y) \gets A(\gamma(y) - \bar{\gamma}_0), \quad l(x) \gets A^{-T} \lambda(x)$$
  then the Euclidian inner product in the transformed spaces is the causal inner product.
- $\langle .,. \rangle_C$ is a causal inner product if $\langle \bar{\gamma}_W, \bar{\gamma}_Z \rangle_C = 0$ whenever $W$ and $Z$ are causally separable.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Defining the Causal Inner Product

- The Riesz isomorphism between the embedding and unembedding spaces is simply the usual vector transpose operation, ie, $\langle \bar{\gamma}_W,. \rangle_C = (\bar{\lambda}_W)^T.$
  - Another way to see it: we can map the embedding space to the unembedding space by mapping each $\bar{\gamma}_W$ to a linear function according to $\bar{\gamma}_W \mapsto \langle \bar{\gamma}_W,. \rangle_C$.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Defining the Causal Inner Product

- Let $M$ be a symmetric positive definite matrix.

- The causal inner product is defined as $\langle \bar{\gamma}, \bar{\gamma}' \rangle_C = \bar{\gamma}^\top M \bar{\gamma}'$.

- If the set of concepts $\{W_k\}_{k=1}^d$ forms a basis $G = [\bar{\gamma}_{W_1}, \dots, \bar{\gamma}_{W_d}]$, then under Assumption 3.3 (causally separable concepts representations are independent):
$$M^{-1} = GG^\top \quad \text{and} \quad G^\top \text{Cov}(\gamma)^{-1} G = D$$
where $ D $ is a diagonal matrix with positive entries.

## Canonical Choice: $D = I_d$

- In experiments, they set: $D = I_d \quad \Rightarrow \quad M = \text{Cov}(\gamma)^{-1}$

- This gives a closed-form inner product:
$$\langle \bar{\gamma}, \bar{\gamma}' \rangle_C := \bar{\gamma}^\top \text{Cov}(\gamma)^{-1} \bar{\gamma}'$$


# General Concepts and Hierarchical Structure

## Binary and Categorical Concepts

![](binary_concepts.png)

::: footer
Source: [Author's presentation](https://kihopark.github.io/files/ICML%202024%20Workshop%20keynote.pdf)
:::

## Hierarchical Structure of Concepts
::: {.fragment}
- Each attribute $w$ is associated to a set of tokens $\mathcal{Y}(w)$ that have the attribute. 
  - For example, $\mathcal{Y}(\texttt{mammal}) =$ {"dogs", "cats", "tiger", ...}.
:::
::: {.fragment}
- A value $z$ is subordinate to a value $w$ (denoted by $z \prec w$) if $\mathcal{Y}(z) \subseteq \mathcal{Y}(w)$. 
:::
::: {.fragment}
- A categorical concept $Z \in \{z_0, \ldots, z_{n-1}\}$ is subordinate to a categorical concept $W \in \{w_0, \ldots, w_{m-1}\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.
:::

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Hierarchical Structure of Concepts: concrete examples

- The binary contrast \ $Z = \text{dog} \implies \text{cat}$ is subordinate to the binary feature $W = \{\text{is_mammal, not_mammal}\}$, ie, $Z \prec W$.
- The binary contrast \ $Z' = \text{parrot} \implies \text{eagle}$ is subordinate to the categorical concept  $W' = \{\text{mammal, bird, fish}\}$, ie, $Z' \prec W'$.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Linear Representation $\bar{l}_W$ of a Binary Concept

Desideratum: If a linear representation exists, moving the representation in this direction should modify the probability of the target concept **in isolation**.

$$P(W = 1\ |\ l + \alpha \bar{l}_W) > P (W = 1\ |\ l)$$

$$P(Z\ |\ l + \alpha \bar{l}_W) = P(Z\ |\ l)$$

For all contexts $l, \alpha > 0$. $Z$ is subortinate to or causally separable with $W$.

::: footer
Source: [Author's presentation](https://kihopark.github.io/files/ICML%202024%20Workshop%20keynote.pdf)
:::

# Representations of Complex Concepts

## High Level Strategy

1. Show how to represent binary features as vectors.
2. Show how geometry encodes semantic composition.
3. Use this to contruct representations of complex concepts.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Vector representation of binary features

- Assume that there is a linear representation (normalized direction) $\bar{\ell}_W$  of a binary feature $W$ for an attribute $w$. Then, there is a constant $b_w > 0$ such that:

::: {.r-stack}
$\left\{
\begin{aligned}
\bar{\ell}_W^\top g(y) &= b_w && \text{if } y \in \mathcal{Y}(w) \\
\bar{\ell}_W^\top g(y) &= 0   && \text{if } y \notin \mathcal{Y}(w)
\end{aligned}
\right.$
:::

- Intuition: if a perfect linear representation of the *animal* feature exists, then every token having the animal attribute has roughly the same inner product with the representation vector: "cat" is as much *animal* as "dog".

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Binary Contrasts Are Vector Differences of Binary Features

- Let $w_0 \Rightarrow w_1$ be a binary contrast, and suppose there exist vector representations $\bar{\ell}_{w_0}$ and $\bar{\ell}_{w_1}$. Then, the difference $\bar{\ell}_{w_1} - \bar{\ell}_{w_0}$ is a linear representation $\bar{\ell}_{w_0 \Rightarrow w_1}$.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Categorical Concepts Representation

- Vector space operations can be used to construct representations of **categorical concepts**, e.g.,  
{mammal, reptile, bird, fish}.
- The polytope representation of a categorical concept $W = \{w_0,\cdots , w_{kâˆ’1} \}$ is the
convex hull of the vector representations of the elements of the concept.

## Categorical Concepts Representation: Convex Hull

```{python}
#| label: fig-polar
#| fig-cap: "Convex Hull of Random Points"
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull

# Generate random 2D points
np.random.seed(42)
points = np.random.rand(20, 2)

# Compute the convex hull
hull = ConvexHull(points)

# Plot
plt.figure(figsize=(6, 6))
plt.plot(points[:,0], points[:,1], 'o', markersize=8, markerfacecolor='lightgray', markeredgecolor='black')

# Draw the convex hull lines
for simplex in hull.simplices:
    plt.plot(points[simplex, 0], points[simplex, 1], 'b-', linewidth=2)

plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], 'o', markeredgewidth=2, markeredgecolor='black', markerfacecolor='white')  # hull points
plt.axis('equal')
plt.axis('off')
plt.tight_layout()
plt.show()
```


## Hierarchical Orthogonality

Suppose there exist the vector representations for all the following binary features. Then, we have that:  

- (a) $\bar{l}_w \perp \bar{l}_z - \bar{l}_w$ for $z \prec w$.

- (d) $\bar{l}_{w_1} - \bar{l}_{w_0} \perp \bar{l}_{w_2} - \bar{l}_{w_1}$ for $w_2 \prec w_1 \prec w_0$.

Intuition: Subtracting the parent of the feature gives orthogonality.

## Hierarchical Orthogonality

b. $\bar{l}_w \perp \bar{l}_{z_0} - \bar{l}_{z_1}$ for $Z \in \{z_0, z_1\}$ subordinate to $W \in \{\textit{not_w}, \textit{is_w}\}$.

![](fig2-b.png){fig-align="center"}

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Hierarchical Orthogonality

c.  $\bar{l}_{w_1} - \bar{l}_{w_0} \perp \bar{l}_{z_0} - \bar{l}_{z_1}$ for $Z \in \{z_0, z_1\}$ subordinate to $W \in \{w_0, w_1\}$.  

![](fig2-c.png){fig-align="center"}

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Estimating Attribute Vectors $\bar{\ell}_w$

* **Goal:** Estimate a vector representation $\bar{\ell}_w$ for each attribute $w$.
* **Approach:** Utilize vocabulary sets $\mathcal{Y}(w)$ associated with the attribute $w$.
* The vector $\bar{\ell}_w$ should satisfy two key properties (based on their theoretical results).

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

---

## Desired Properties & LDA

1.  **Separation:** When the full vocabulary is projected onto $\bar{\ell}_w$, words in $\mathcal{Y}(w)$ should be clearly separated from the rest.
2.  **Consistency:** Projections of unembedding vectors $y \in \mathcal{Y}(w)$ should have approximately the same value (i.e., small variance).

* **Method:** They use Linear Discriminant Analysis (LDA) to find projection directions that minimize within-class variance and maximize between-class variance.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

---

## Formal Estimation

- The vector representation $\bar{\ell}_w$ for a binary feature $W$ corresponding to attribute $w$ is estimated as $\bar{\ell}_w = (\tilde{g}_w^\top \mathbb{E}(g_w)) \tilde{g}_w$.
- $\tilde{g}_w$ is defined as:
    $$
    \tilde{g}_w = \frac{\text{Cov}(g_w)^\dagger \mathbb{E}(g_w)}{\|\text{Cov}(g_w)^\dagger \mathbb{E}(g_w)\|_2},
    $$
    where $g_w$ is an unembedding vector of a word sampled uniformly from the vocabulary set $\mathcal{Y}$.

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

# Some Results

## Setup

- Gemma-2B model.
- WordNet Dataset:

![Subtree in WordNet noun hierarchy for descendants of animal.](wordnet_animal_structure.png)

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Hierarchical semantics in WordNet are linearly represented in Gemma-2B

- The cosine similarity between the vector representations reflects the WordNet structure.

![Heatmaps for the WordNet.](heatmap.png)

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Hierarchical semantics in WordNet are linearly represented in Gemma-2B

![Heatmaps of the subtree for animal.](heatmaps_wordnet_animal.png)

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Categorical Concepts are Represented as Polytopes in Gemma

![](cat_conceps_poly.png){fig-align="center"}

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

## Overall Structure of the Representation of Hierarchically Related Concepts

![](hierarc_concepts.png){fig-align="center"}

::: footer
Source: Park, K., Choe, Y. J., Jiang, Y., & Veitch, V (2024)
:::

# Q&A

## References

- Park, K., Choe, Y. J., & Veitch, V. (2024, July). The Linear Representation Hypothesis and the Geometry of Large Language Models. In International Conference on Machine Learning (pp. 39643-39666). PMLR.

- Park, K., Choe, Y. J., Jiang, Y., & Veitch, V. The Geometry of Categorical and Hierarchical Concepts in Large Language Models. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models.



